{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openml\n",
    "import joblib\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "from tqdm.notebook import tqdm\n",
    "from copy import deepcopy as copy\n",
    "from joblib import Parallel, delayed\n",
    "from matplotlib import pyplot as plt\n",
    "from xgboost import XGBClassifier as XGBC\n",
    "from sklearn.preprocessing import RobustScaler as RS\n",
    "\n",
    "openml.config.apikey = \"4ef25cfe971a3731fddbe4fb2f6d1d98\"\n",
    "data_folder = \"/data/pereirabarataap/journal/\"\n",
    "pd.set_option(\"max.columns\", 1000)\n",
    "pd.set_option(\"max.rows\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "We use [openML](https://www.openml.org/) to get some (10) datasets for our experiments.\n",
    "\n",
    "The datasets must respect the following:\n",
    "* no missing values\n",
    "* be between 20 to 100 features wide\n",
    "* be between 100 and 5000 instances long\n",
    "* classification task-related\n",
    "    * 2 class-specific\n",
    "    * at least 100 instances per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "**Author**: Space Physics Group, Applied Physics Laboratory, Johns Hopkins University. Donated by Vince Sigillito.  \n",
      "**Source**: [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/ionosphere)  \n",
      "**Please cite**: [UCI](https://archive.ics.uci.edu/ml/citation_policy.html) \n",
      "\n",
      "**Johns Hopkins University Ionosphere database**  \n",
      "This radar data was collected by a system in Goose Bay, Labrador.  This system consists of a phased array of 16 high-frequency antennas with a total transmitted power on the order of 6.4 kilowatts.  See the paper for more details.  \n",
      "\n",
      "### Attribute information\n",
      "Received signals were processed using an autocorrelation function whose arguments are the time of a pulse and the pulse number.  There were 17 pulse numbers for the Goose Bay system.  Instances in this database are described by 2 attributes per pulse number, corresponding to the complex values returned by the function resulting from the complex electromagnetic signal.\n",
      "\n",
      "The targets were free electrons in the ionosphere.  \"Good\" (g) radar returns are those showing evidence of some type of structure in the ionosphere.  \"Bad\" (b) returns are those that do not; their signals pass through the ionosphere.  \n",
      "\n",
      "### Relevant papers  \n",
      "Sigillito, V. G., Wing, S. P., Hutton, L. V., & Baker, K. B. (1989). Classification of radar returns from the ionosphere using neural networks. Johns Hopkins APL Technical Digest, 10, 262-266.\n",
      "\n",
      "\n",
      "\n",
      "1063\n",
      "**Author**: Mike Chapman, NASA  \n",
      "**Source**: [tera-PROMISE](http://openscience.us/repo/defect/mccabehalsted/kc2.html) - 2004  \n",
      "**Please cite**: Sayyad Shirabad, J. and Menzies, T.J. (2005) The PROMISE Repository of Software Engineering Databases. School of Information Technology and Engineering, University of Ottawa, Canada.  \n",
      "  \n",
      "**KC2 Software defect prediction**  \n",
      "One of the NASA Metrics Data Program defect data sets. Data from software for science data processing. Data comes from McCabe and Halstead features extractors of source code.  These features were defined in the 70s in an attempt to objectively characterize code features that are associated with software quality.\n",
      "\n",
      "### Attribute Information  \n",
      "\n",
      "1. loc             : numeric % McCabe's line count of code\n",
      "2. v(g)            : numeric % McCabe \"cyclomatic complexity\"\n",
      "3. ev(g)           : numeric % McCabe \"essential complexity\"\n",
      "4. iv(g)           : numeric % McCabe \"design complexity\"\n",
      "5. n               : numeric % Halstead total operators + operands\n",
      "6. v               : numeric % Halstead \"volume\"\n",
      "7. l               : numeric % Halstead \"program length\"\n",
      "8. d               : numeric % Halstead \"difficulty\"\n",
      "9. i               : numeric % Halstead \"intelligence\"\n",
      "10. e               : numeric % Halstead \"effort\"\n",
      "11. b               : numeric % Halstead \n",
      "12. t               : numeric % Halstead's time estimator\n",
      "13. lOCode          : numeric % Halstead's line count\n",
      "14. lOComment       : numeric % Halstead's count of lines of comments\n",
      "15. lOBlank         : numeric % Halstead's count of blank lines\n",
      "16. lOCodeAndComment: numeric\n",
      "17. uniq_Op         : numeric % unique operators\n",
      "18. uniq_Opnd       : numeric % unique operands\n",
      "19. total_Op        : numeric % total operators\n",
      "20. total_Opnd      : numeric % total operands\n",
      "21. branchCount     : numeric % of the flow graph\n",
      "22. problems        : {false,true} % module has/has not one or more reported defects\n",
      "\n",
      "### Relevant papers  \n",
      "\n",
      "- Shepperd, M. and Qinbao Song and Zhongbin Sun and Mair, C. (2013)\n",
      "Data Quality: Some Comments on the NASA Software Defect Datasets, IEEE Transactions on Software Engineering, 39.\n",
      "\n",
      "- Tim Menzies and Justin S. Di Stefano (2004) How Good is Your Blind Spot Sampling Policy? 2004 IEEE Conference on High Assurance\n",
      "Software Engineering.\n",
      "\n",
      "- T. Menzies and J. DiStefano and A. Orrego and R. Chapman (2004) Assessing Predictors of Software Defects\", Workshop on Predictive Software Models, Chicago\n",
      "\n",
      "\n",
      "\n",
      "1510\n",
      "**Author**: William H. Wolberg, W. Nick Street, Olvi L. Mangasarian    \n",
      "**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)), [University of Wisconsin](http://pages.cs.wisc.edu/~olvi/uwmp/cancer.html) - 1995  \n",
      "**Please cite**: [UCI](https://archive.ics.uci.edu/ml/citation_policy.html)     \n",
      "\n",
      "**Breast Cancer Wisconsin (Diagnostic) Data Set (WDBC).** Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. The target feature records the prognosis (benign (1) or malignant (2)). [Original data available here](ftp://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/) \n",
      "\n",
      "Current dataset was adapted to ARFF format from the UCI version. Sample code ID's were removed.  \n",
      "\n",
      "! Note that there is also a related Breast Cancer Wisconsin (Original) Data Set with a different set of features, better known as [breast-w](https://www.openml.org/d/15).\n",
      "\n",
      "\n",
      "### Feature description  \n",
      "\n",
      "Ten real-valued features are computed for each of 3 cell nuclei, yielding a total of 30 descriptive features. See the papers below for more details on how they were computed. The 10 features (in order) are:  \n",
      "\n",
      "a) radius (mean of distances from center to points on the perimeter)  \n",
      "b) texture (standard deviation of gray-scale values)  \n",
      "c) perimeter  \n",
      "d) area  \n",
      "e) smoothness (local variation in radius lengths)  \n",
      "f) compactness (perimeter^2 / area - 1.0)  \n",
      "g) concavity (severity of concave portions of the contour)  \n",
      "h) concave points (number of concave portions of the contour)  \n",
      "i) symmetry  \n",
      "j) fractal dimension (\"coastline approximation\" - 1)  \n",
      "\n",
      "### Relevant Papers   \n",
      "\n",
      "W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on Electronic Imaging: Science and Technology, volume 1905, pages 861-870, San Jose, CA, 1993. \n",
      "\n",
      "O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and prognosis via linear programming. Operations Research, 43(4), pages 570-577, July-August 1995.\n",
      "\n",
      "\n",
      "\n",
      "40589\n",
      "Multi-label dataset. Audio dataset (emotions) consists of 593 musical files with 6 clustered emotional labels and 72 predictors. Each song can be labeled with one or more of the labels {amazed-surprised, happy-pleased, relaxing-calm, quiet-still, sad-lonely, angry-aggressive}.\n",
      "\n",
      "\n",
      "\n",
      "40705\n",
      "**Author**: Ron Kohavi   \n",
      "**Source**: [SGI.com tech archive](http://www.sgi.com/tech/mlc/db/) (no longer available, [copy on GitHub](https://github.com/acefoxy/DataScience/blob/973d9239ca3190487204ce8037a1d3c8689f95dd/week2/www.sgi.com/tech/mlc/db/tokyo1.names\n",
      ")), [PMLB](https://github.com/EpistasisLab/penn-ml-benchmarks/tree/master/datasets/classification/tokyo1)  \n",
      "**Please cite**:  \n",
      "\n",
      "**Tokyo SGI Server Performance Data**  \n",
      "This is Performance co-pilot (PCP) data for the Tokyo server at Silicon Graphics International (SGI). \n",
      "It characterizes the server performance as either `good` (1) or `bad` (0).\n",
      "\n",
      "The instances are measurements generated by the PCP software every five seconds. See the [PCP manual](http://www.irix7.com/techpubs/007-2614-001.pdf) for further details.\n",
      "\n",
      "### Attribute Information  \n",
      "The attributes are interpreted as follows:  \n",
      "- runq: Average number of runnable processes in main memory (mem) and in swap memory (swp) during the interval.\n",
      "- memory: The free column indicates average free memory during the interval, in Kilobytes. The page column is the average number of page out operations per second during the interval. I/O operations caused by these page-out operations are included in the write I/O rate.\n",
      "- system: System call rate (scall), context switch rate (ctxsw) and interrupt rate (intr). Rates are expressed as average operations per second during the interval.\n",
      "- disks: Aggregated physical read (rd) and write (wr) rates over all disks, expressed as physical I/O operations issued per second during the interval. These rates are independent of the I/O block size.\n",
      "- cpu: Percentage of CPU time spent executing user code (usr), system and interrupt code (sys), idle loop (idl) and idle waiting for resources, typically disk I/O (wt).\n",
      "\n",
      "\n",
      "\n",
      "31\n",
      "**Author**: Dr. Hans Hofmann  \n",
      "**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)) - 1994    \n",
      "**Please cite**: [UCI](https://archive.ics.uci.edu/ml/citation_policy.html)\n",
      "\n",
      "**German Credit data**  \n",
      "This dataset classifies people described by a set of attributes as good or bad credit risks.\n",
      "\n",
      "This dataset comes with a cost matrix: \n",
      "``` \n",
      "      Good  Bad (predicted)  \n",
      "Good   0    1   (actual)  \n",
      "Bad    5    0  \n",
      "```\n",
      "\n",
      "It is worse to class a customer as good when they are bad (5), than it is to class a customer as bad when they are good (1).  \n",
      "\n",
      "### Attribute description  \n",
      "\n",
      "1. Status of existing checking account, in Deutsche Mark.  \n",
      "2. Duration in months  \n",
      "3. Credit history (credits taken, paid back duly, delays, critical accounts)  \n",
      "4. Purpose of the credit (car, television,...)  \n",
      "5. Credit amount  \n",
      "6. Status of savings account/bonds, in Deutsche Mark.  \n",
      "7. Present employment, in number of years.  \n",
      "8. Installment rate in percentage of disposable income  \n",
      "9. Personal status (married, single,...) and sex  \n",
      "10. Other debtors / guarantors  \n",
      "11. Present residence since X years  \n",
      "12. Property (e.g. real estate)  \n",
      "13. Age in years  \n",
      "14. Other installment plans (banks, stores)  \n",
      "15. Housing (rent, own,...)  \n",
      "16. Number of existing credits at this bank  \n",
      "17. Job  \n",
      "18. Number of people being liable to provide maintenance for  \n",
      "19. Telephone (yes,no)  \n",
      "20. Foreign worker (yes,no)\n",
      "\n",
      "\n",
      "\n",
      "1547\n",
      "**Author**: Ray. J. Hickey   \n",
      "**Source**: UCI  \n",
      "**Please cite**:   \n",
      "\n",
      "* Dataset Title:  \n",
      "\n",
      "AutoUniv Dataset  \n",
      "data problem: autoUniv-au1-1000   \n",
      "\n",
      "* Abstract:   \n",
      "\n",
      "AutoUniv is an advanced data generator for classifications tasks. The aim is to reflect the nuances and heterogeneity of real data. Data can be generated in .csv, ARFF or C4.5 formats.\n",
      "\n",
      "* Source:  \n",
      "\n",
      "AutoUniv was developed by Ray. J. Hickey. Email: ray.j.hickey '@' gmail.com \n",
      "AutoUniv web-site: http://sites.google.com/site/autouniv/.\n",
      "\n",
      "\n",
      "* Data Set Information:\n",
      "\n",
      "The user first creates a classification model and then generates classified examples from it. To create a model, the following are specified: the number of attributes (up to 1000) and their type (discrete or continuous), the number of classes (up to 10), the complexity of the underlying rules and the noise level. AutoUniv then produces a model through a process of constrained randomised search to satisfy the user's requirements. A model can have up to 3000 rules. Rare class models can be designed. A sequence of models can be designed to reflect concept and/or population drift. \n",
      "\n",
      "AutoUniv creates three text files for a model: a Prolog specification of the model used to generate examples (.aupl); a user-friendly statement of the classification rules in an 'if ... then' format (.aurules); a statistical summary of the main properties of the model, including its Bayes rate (.auprops).\n",
      "\n",
      "\n",
      "* Attribute Information: \n",
      "\n",
      "Attributes may be discrete with up to 10 values or continuous. A discrete attribute can be nominal with values v1, v2, v3 ... or integer with values 0, 1, 2 , ... .\n",
      "\n",
      "\n",
      "* Relevant Papers:\n",
      "\n",
      "Marrs, G, Hickey, RJ and Black, MM (2010) Modeling the example life-cycle in an online classification learner. In Proceedings of HaCDAIS 2010: International Workshop on Handling Concept Drift in Adaptive Information Systems. \n",
      "[Web Link]#proc . \n",
      "\n",
      "Marrs, G, Hickey, RJ and Black, MM (2010) The Impact of Latency on Online Classification Learning with Concept Drift. In Y. Bi and M.A. Williams (Eds.): KSEM 2010, LNAI 6291, Springer-Verlag, Berlin, pp. 459â€“469. \n",
      "\n",
      "Hickey, RJ (2007) Structure and Majority Classes in Decision Tree Learning. Journal of Machine Learning Research, 8, pp. 1747-1768.\n",
      "\n",
      "\n",
      "\n",
      "1444\n",
      "**Author**: Hans Bauer Jesus\",\"Deter Bergman  \n",
      "**Source**: Unknown - Date unknown  \n",
      "**Please cite**:   \n",
      "\n",
      "Pizza cutter 3\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1494\n",
      "**Author**: Kamel Mansouri, Tine Ringsted, Davide Ballabio  \n",
      "**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation)  \n",
      "**Please cite**: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878 \n",
      "\n",
      "\n",
      "QSAR biodegradation Data Set \n",
      "\n",
      "* Abstract: \n",
      "\n",
      "Data set containing values for 41 attributes (molecular descriptors) used to classify 1055 chemicals into 2 classes (ready and not ready biodegradable).\n",
      "\n",
      "\n",
      "* Source:\n",
      "\n",
      "Kamel Mansouri, Tine Ringsted, Davide Ballabio (davide.ballabio '@' unimib.it), Roberto Todeschini, Viviana Consonni, Milano Chemometrics and QSAR Research Group (http://michem.disat.unimib.it/chm/), UniversitÃ  degli Studi Milano â€“ Bicocca, Milano (Italy)\n",
      "\n",
      "\n",
      "* Data Set Information:\n",
      "\n",
      "The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group (UniversitÃ  degli Studi Milano â€“ Bicocca, Milano, Italy). The research leading to these results has received funding from the European Communityâ€™s Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project. \n",
      "The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.\n",
      "\n",
      "\n",
      "* Attribute Information:\n",
      "\n",
      "41 molecular descriptors and 1 experimental class: \n",
      "1) SpMax_L: Leading eigenvalue from Laplace matrix \n",
      "2) J_Dz(e): Balaban-like index from Barysz matrix weighted by Sanderson electronegativity \n",
      "3) nHM: Number of heavy atoms \n",
      "4) F01[N-N]: Frequency of N-N at topological distance 1 \n",
      "5) F04[C-N]: Frequency of C-N at topological distance 4 \n",
      "6) NssssC: Number of atoms of type ssssC \n",
      "7) nCb-: Number of substituted benzene C(sp2) \n",
      "8) C%: Percentage of C atoms \n",
      "9) nCp: Number of terminal primary C(sp3) \n",
      "10) nO: Number of oxygen atoms \n",
      "11) F03[C-N]: Frequency of C-N at topological distance 3 \n",
      "12) SdssC: Sum of dssC E-states \n",
      "13) HyWi_B(m): Hyper-Wiener-like index (log function) from Burden matrix weighted by mass \n",
      "14) LOC: Lopping centric index \n",
      "15) SM6_L: Spectral moment of order 6 from Laplace matrix \n",
      "16) F03[C-O]: Frequency of C - O at topological distance 3 \n",
      "17) Me: Mean atomic Sanderson electronegativity (scaled on Carbon atom) \n",
      "18) Mi: Mean first ionization potential (scaled on Carbon atom) \n",
      "19) nN-N: Number of N hydrazines \n",
      "20) nArNO2: Number of nitro groups (aromatic) \n",
      "21) nCRX3: Number of CRX3 \n",
      "22) SpPosA_B(p): Normalized spectral positive sum from Burden matrix weighted by polarizability \n",
      "23) nCIR: Number of circuits \n",
      "24) B01[C-Br]: Presence/absence of C - Br at topological distance 1 \n",
      "25) B03[C-Cl]: Presence/absence of C - Cl at topological distance 3 \n",
      "26) N-073: Ar2NH / Ar3N / Ar2N-Al / R..N..R \n",
      "27) SpMax_A: Leading eigenvalue from adjacency matrix (Lovasz-Pelikan index) \n",
      "28) Psi_i_1d: Intrinsic state pseudoconnectivity index - type 1d \n",
      "29) B04[C-Br]: Presence/absence of C - Br at topological distance 4 \n",
      "30) SdO: Sum of dO E-states \n",
      "31) TI2_L: Second Mohar index from Laplace matrix \n",
      "32) nCrt: Number of ring tertiary C(sp3) \n",
      "33) C-026: R--CX--R \n",
      "34) F02[C-N]: Frequency of C - N at topological distance 2 \n",
      "35) nHDon: Number of donor atoms for H-bonds (N and O) \n",
      "36) SpMax_B(m): Leading eigenvalue from Burden matrix weighted by mass \n",
      "37) Psi_i_A: Intrinsic state pseudoconnectivity index - type S average \n",
      "38) nN: Number of Nitrogen atoms \n",
      "39) SM6_B(m): Spectral moment of order 6 from Burden matrix weighted by mass \n",
      "40) nArCOOR: Number of esters (aromatic) \n",
      "41) nX: Number of halogen atoms \n",
      "42) experimental class: ready biodegradable (RB) and not ready biodegradable (NRB)\n",
      "\n",
      "\n",
      "* Relevant Papers:\n",
      "\n",
      "Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878\n",
      "\n",
      "\n",
      "\n",
      "1453\n",
      "**Author**: Hans Bauer Jesus\",\"Deter Bergman  \n",
      "**Source**: Unknown - Date unknown  \n",
      "**Please cite**:   \n",
      "\n",
      "pie chart 3\n",
      "\n",
      "\n",
      "\n",
      "1049\n",
      "**Author**: Mike Chapman, NASA  \n",
      "**Source**: [tera-PROMISE](http://openscience.us/repo/defect/mccabehalsted/pc1.html) - 2004  \n",
      "**Please cite**: Sayyad Shirabad, J. and Menzies, T.J. (2005) The PROMISE Repository of Software Engineering Databases. School of Information Technology and Engineering, University of Ottawa, Canada.  \n",
      "  \n",
      "**PC4 Software defect prediction**  \n",
      "One of the NASA Metrics Data Program defect data sets. Data from flight software for earth orbiting satellite. Data comes from McCabe and Halstead features extractors of source code.  These features were defined in the 70s in an attempt to objectively characterize code features that are associated with software quality.\n",
      "\n",
      "### Relevant papers  \n",
      "\n",
      "- Shepperd, M. and Qinbao Song and Zhongbin Sun and Mair, C. (2013)\n",
      "Data Quality: Some Comments on the NASA Software Defect Datasets, IEEE Transactions on Software Engineering, 39.\n",
      "\n",
      "- Tim Menzies and Justin S. Di Stefano (2004) How Good is Your Blind Spot Sampling Policy? 2004 IEEE Conference on High Assurance\n",
      "Software Engineering.\n",
      "\n",
      "- T. Menzies and J. DiStefano and A. Orrego and R. Chapman (2004) Assessing Predictors of Software Defects\", Workshop on Predictive Software Models, Chicago\n",
      "\n",
      "\n",
      "\n",
      "1050\n",
      "**Author**: Mike Chapman, NASA  \n",
      "**Source**: [tera-PROMISE](http://openscience.us/repo/defect/mccabehalsted/pc3.html) - 2004  \n",
      "**Please cite**: Sayyad Shirabad, J. and Menzies, T.J. (2005) The PROMISE Repository of Software Engineering Databases. School of Information Technology and Engineering, University of Ottawa, Canada.  \n",
      "  \n",
      "**PC3 Software defect prediction**  \n",
      "One of the NASA Metrics Data Program defect data sets. Data from flight software for earth orbiting satellite. Data comes from McCabe and Halstead features extractors of source code.  These features were defined in the 70s in an attempt to objectively characterize code features that are associated with software quality.\n",
      "\n",
      "### Relevant papers  \n",
      "\n",
      "- Shepperd, M. and Qinbao Song and Zhongbin Sun and Mair, C. (2013)\n",
      "Data Quality: Some Comments on the NASA Software Defect Datasets, IEEE Transactions on Software Engineering, 39.\n",
      "\n",
      "- Tim Menzies and Justin S. Di Stefano (2004) How Good is Your Blind Spot Sampling Policy? 2004 IEEE Conference on High Assurance\n",
      "Software Engineering.\n",
      "\n",
      "- T. Menzies and J. DiStefano and A. Orrego and R. Chapman (2004) Assessing Predictors of Software Defects\", Workshop on Predictive Software Models, Chicago\n",
      "\n",
      "\n",
      "\n",
      "1504\n",
      "**Author**: Semeion, Research Center of Sciences of Communication, Rome, Italy.     \n",
      "**Source**: [UCI](http://archive.ics.uci.edu/ml/datasets/steel+plates+faults)     \n",
      "**Please cite**: Dataset provided by Semeion, Research Center of Sciences of Communication, Via Sersale 117, 00128, Rome, Italy.  \n",
      "\n",
      "**Steel Plates Faults Data Set**  \n",
      "A dataset of steel plates' faults, classified into 7 different types. The goal was to train machine learning for automatic pattern recognition.\n",
      "\n",
      "The dataset consists of 27 features describing each fault (location, size, ...) and 7 binary features indicating the type of fault (on of 7: Pastry, Z_Scratch, K_Scatch, Stains, Dirtiness, Bumps, Other_Faults). The latter is commonly used as a binary classification target ('common' or 'other' fault.)\n",
      "\n",
      "### Attribute Information  \n",
      "* V1: X_Minimum  \n",
      "* V2: X_Maximum  \n",
      "* V3: Y_Minimum  \n",
      "* V4: Y_Maximum  \n",
      "* V5: Pixels_Areas  \n",
      "* V6: X_Perimeter  \n",
      "* V7: Y_Perimeter  \n",
      "* V8: Sum_of_Luminosity  \n",
      "* V9: Minimum_of_Luminosity  \n",
      "* V10: Maximum_of_Luminosity  \n",
      "* V11: Length_of_Conveyer  \n",
      "* V12: TypeOfSteel_A300  \n",
      "* V13: TypeOfSteel_A400  \n",
      "* V14: Steel_Plate_Thickness  \n",
      "* V15: Edges_Index  \n",
      "* V16: Empty_Index  \n",
      "* V17: Square_Index  \n",
      "* V18: Outside_X_Index  \n",
      "* V19: Edges_X_Index  \n",
      "* V20: Edges_Y_Index  \n",
      "* V21: Outside_Global_Index  \n",
      "* V22: LogOfAreas  \n",
      "* V23: Log_X_Index  \n",
      "* V24: Log_Y_Index  \n",
      "* V25: Orientation_Index  \n",
      "* V26: Luminosity_Index  \n",
      "* V27: SigmoidOfAreas  \n",
      "* V28: Pastry  \n",
      "* V29: Z_Scratch  \n",
      "* V30: K_Scatch  \n",
      "* V31: Stains  \n",
      "* V32: Dirtiness  \n",
      "* V33: Bumps  \n",
      "* Class: Other_Faults  \n",
      "\n",
      "### Relevant Papers  \n",
      "1.M Buscema, S Terzi, W Tastle, A New Meta-Classifier,in NAFIPS 2010, Toronto (CANADA),26-28 July 2010, 978-1-4244-7858-6/10 Â©2010 IEEE  \n",
      "2.M Buscema, MetaNet: The Theory of Independent Judges, in Substance Use & Misuse, 33(2), 439-461,1998\n",
      "\n",
      "\n",
      "\n",
      "995\n",
      "**Author**:   \n",
      "**Source**: Unknown - Date unknown  \n",
      "**Please cite**:   \n",
      "\n",
      "Binarized version of the original data set (see version 1). The multi-class target feature is converted to a two-class nominal target feature by re-labeling the majority class as positive ('P') and all others as negative ('N'). Originally converted by Quan Sun.\n",
      "\n",
      "\n",
      "\n",
      "971\n",
      "**Author**:   \n",
      "**Source**: Unknown - Date unknown  \n",
      "**Please cite**:   \n",
      "\n",
      "Binarized version of the original data set (see version 1). The multi-class target feature is converted to a two-class nominal target feature by re-labeling the majority class as positive ('P') and all others as negative ('N'). Originally converted by Quan Sun.\n",
      "\n",
      "\n",
      "\n",
      "1020\n",
      "**Author**:   \n",
      "**Source**: Unknown - Date unknown  \n",
      "**Please cite**:   \n",
      "\n",
      "Binarized version of the original data set (see version 1). The multi-class target feature is converted to a two-class nominal target feature by re-labeling the majority class as positive ('P') and all others as negative ('N'). Originally converted by Quan Sun.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 1067: kc1 to file /home/pereirabarataap/.openml/cache/org/openml/www/datasets/1067/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1067\n",
      "**Author**: Mike Chapman, NASA  \n",
      "**Source**: [tera-PROMISE](http://openscience.us/repo/defect/mccabehalsted/kc1.html) - 2004  \n",
      "**Please cite**: Sayyad Shirabad, J. and Menzies, T.J. (2005) The PROMISE Repository of Software Engineering Databases. School of Information Technology and Engineering, University of Ottawa, Canada.  \n",
      "  \n",
      "**KC1 Software defect prediction**  \n",
      "One of the NASA Metrics Data Program defect data sets. Data from software for storage management for receiving and processing ground data. Data comes from McCabe and Halstead features extractors of source code.  These features were defined in the 70s in an attempt to objectively characterize code features that are associated with software quality.\n",
      "\n",
      "### Attribute Information  \n",
      "\n",
      "1. loc             : numeric % McCabe's line count of code\n",
      "2. v(g)            : numeric % McCabe \"cyclomatic complexity\"\n",
      "3. ev(g)           : numeric % McCabe \"essential complexity\"\n",
      "4. iv(g)           : numeric % McCabe \"design complexity\"\n",
      "5. n               : numeric % Halstead total operators + operands\n",
      "6. v               : numeric % Halstead \"volume\"\n",
      "7. l               : numeric % Halstead \"program length\"\n",
      "8. d               : numeric % Halstead \"difficulty\"\n",
      "9. i               : numeric % Halstead \"intelligence\"\n",
      "10. e               : numeric % Halstead \"effort\"\n",
      "11. b               : numeric % Halstead \n",
      "12. t               : numeric % Halstead's time estimator\n",
      "13. lOCode          : numeric % Halstead's line count\n",
      "14. lOComment       : numeric % Halstead's count of lines of comments\n",
      "15. lOBlank         : numeric % Halstead's count of blank lines\n",
      "16. lOCodeAndComment: numeric\n",
      "17. uniq_Op         : numeric % unique operators\n",
      "18. uniq_Opnd       : numeric % unique operands\n",
      "19. total_Op        : numeric % total operators\n",
      "20. total_Opnd      : numeric % total operands\n",
      "21. branchCount     : numeric % of the flow graph\n",
      "22. problems        : {false,true} % module has/has not one or more reported defects\n",
      "\n",
      "### Relevant papers  \n",
      "\n",
      "- Shepperd, M. and Qinbao Song and Zhongbin Sun and Mair, C. (2013)\n",
      "Data Quality: Some Comments on the NASA Software Defect Datasets, IEEE Transactions on Software Engineering, 39.\n",
      "\n",
      "- Tim Menzies and Justin S. Di Stefano (2004) How Good is Your Blind Spot Sampling Policy? 2004 IEEE Conference on High Assurance\n",
      "Software Engineering.\n",
      "\n",
      "- T. Menzies and J. DiStefano and A. Orrego and R. Chapman (2004) Assessing Predictors of Software Defects\", Workshop on Predictive Software Models, Chicago\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 958: segment to file /home/pereirabarataap/.openml/cache/org/openml/www/datasets/958/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "958\n",
      "**Author**:   \n",
      "**Source**: Unknown - Date unknown  \n",
      "**Please cite**:   \n",
      "\n",
      "Binarized version of the original data set (see version 1). The multi-class target feature is converted to a two-class nominal target feature by re-labeling the majority class as positive ('P') and all others as negative ('N'). Originally converted by Quan Sun.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 1487: ozone-level-8hr to file /home/pereirabarataap/.openml/cache/org/openml/www/datasets/1487/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1487\n",
      "**Author**: Kun Zhang, Wei Fan, XiaoJing Yuan\n",
      "\n",
      "**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/ozone+level+detection)\n",
      "\n",
      "**Please cite**:   \n",
      "\n",
      "Forecasting skewed biased stochastic ozone days: analyses, solutions and beyond, Knowledge and Information Systems, Vol. 14, No. 3, 2008. \n",
      "\n",
      "\n",
      "1 . Abstract: \n",
      "Two ground ozone level data sets are included in this collection. One is the eight hour peak set (eighthr.data), the other is the one hour peak set (onehr.data). Those data were collected from 1998 to 2004 at the Houston, Galveston and Brazoria area.\n",
      "\n",
      "2. Source:\n",
      "\n",
      "Kun Zhang, zhang.kun05 '@' gmail.com, Department of Computer Science, Xavier University of Lousiana \n",
      "Wei Fan, wei.fan '@' gmail.com, IBM T.J.Watson Research \n",
      "XiaoJing Yuan, xyuan '@' uh.edu, Engineering Technology Department, College of Technology, University of Houston \n",
      "\n",
      "\n",
      "3. Data Set Information:\n",
      "\n",
      "All the attribute start with T means the temperature measured at different time throughout the day; and those starts with WS indicate the wind speed at various time. \n",
      "\n",
      "WSR_PK: continuous. peek wind speed -- resultant (meaning average of wind vector) \n",
      "WSR_AV: continuous. average wind speed \n",
      "T_PK: continuous. Peak T \n",
      "T_AV: continuous. Average T \n",
      "T85: continuous. T at 850 hpa level (or about 1500 m height) \n",
      "RH85: continuous. Relative Humidity at 850 hpa \n",
      "U85: continuous. (U wind - east-west direction wind at 850 hpa) \n",
      "V85: continuous. V wind - N-S direction wind at 850 \n",
      "HT85: continuous. Geopotential height at 850 hpa, it is about the same as height at low altitude \n",
      "T70: continuous. T at 700 hpa level (roughly 3100 m height) \n",
      "\n",
      "RH70: continuous. \n",
      "U70: continuous. \n",
      "V70: continuous. \n",
      "HT70: continuous. \n",
      "\n",
      "T50: continuous. T at 500 hpa level (roughly at 5500 m height) \n",
      "\n",
      "RH50: continuous. \n",
      "U50: continuous. \n",
      "V50: continuous. \n",
      "HT50: continuous. \n",
      "\n",
      "KI: continuous. K-Index [Web Link] \n",
      "TT: continuous. T-Totals [Web Link] \n",
      "SLP: continuous. Sea level pressure \n",
      "SLP_: continuous. SLP change from previous day \n",
      "\n",
      "Precp: continuous. -- precipitation\n",
      "\n",
      "\n",
      "4. Attribute Information:\n",
      "\n",
      "The following are specifications for several most important attributes that are highly valued by Texas Commission on Environmental Quality (TCEQ). More details can be found in the two relevant papers. \n",
      "\n",
      "O 3 - Local ozone peak prediction \n",
      "Upwind - Upwind ozone background level \n",
      "EmFactor - Precursor emissions related factor \n",
      "Tmax - Maximum temperature in degrees F \n",
      "Tb - Base temperature where net ozone production begins (50 F) \n",
      "SRd - Solar radiation total for the day \n",
      "WSa - Wind speed near sunrise (using 09-12 UTC forecast mode) \n",
      "WSp - Wind speed mid-day (using 15-21 UTC forecast mode) \n",
      "\n",
      "\n",
      "5. Relevant Papers:\n",
      "\n",
      "Forecasting skewed biased stochastic ozone days: analyses, solutions and beyond, Knowledge and Information Systems, Vol. 14, No. 3, 2008. \n",
      "\n",
      "It Discusses details about the dataset, its use as well as various experiments (both cross-validation and streaming) using many state-of-the-art methods. \n",
      "A shorter version of the paper (does not contain some detailed experiments as the journal paper above) is in: \n",
      "Forecasting Skewed Biased Stochastic Ozone Days: Analyses and Solutions. ICDM 2006: 753-764\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 953: splice to file /home/pereirabarataap/.openml/cache/org/openml/www/datasets/953/dataset.pkl.py3\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "953\n",
      "**Author**:   \n",
      "**Source**: Unknown - Date unknown  \n",
      "**Please cite**:   \n",
      "\n",
      "Binarized version of the original data set (see version 1). The multi-class target feature is converted to a two-class nominal target feature by re-labeling the majority class as positive ('P') and all others as negative ('N'). Originally converted by Quan Sun.\n",
      "\n",
      "\n",
      "\n",
      "3\n",
      "**Author**: Alen Shapiro\n",
      "**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Chess+(King-Rook+vs.+King-Pawn))    \n",
      "**Please cite**: [UCI citation policy](https://archive.ics.uci.edu/ml/citation_policy.html)  \n",
      "\n",
      "1. Title: Chess End-Game -- King+Rook versus King+Pawn on a7\n",
      "    (usually abbreviated KRKPA7).  The pawn on a7 means it is one square\n",
      "    away from queening.  It is the King+Rook's side (white) to move.\n",
      " \n",
      " 2. Sources:\n",
      "     (a) Database originally generated and described by Alen Shapiro.\n",
      "     (b) Donor/Coder: Rob Holte (holte@uottawa.bitnet).  The database\n",
      "         was supplied to Holte by Peter Clark of the Turing Institute\n",
      "         in Glasgow (pete@turing.ac.uk).\n",
      "     (c) Date: 1 August 1989\n",
      " \n",
      " 3. Past Usage:\n",
      "      - Alen D. Shapiro (1983,1987), \"Structured Induction in Expert Systems\",\n",
      "        Addison-Wesley.  This book is based on Shapiro's Ph.D. thesis (1983)\n",
      "        at the University of Edinburgh entitled \"The Role of Structured\n",
      "        Induction in Expert Systems\".\n",
      "      - Stephen Muggleton (1987), \"Structuring Knowledge by Asking Questions\",\n",
      "        pp.218-229 in \"Progress in Machine Learning\", edited by I. Bratko\n",
      "        and Nada Lavrac, Sigma Press, Wilmslow, England  SK9 5BB.\n",
      "      - Robert C. Holte, Liane Acker, and Bruce W. Porter (1989),\n",
      "        \"Concept Learning and the Problem of Small Disjuncts\",\n",
      "        Proceedings of IJCAI.  Also available as technical report AI89-106,\n",
      "        Computer Sciences Department, University of Texas at Austin,\n",
      "        Austin, Texas 78712.\n",
      " \n",
      " 4. Relevant Information:\n",
      "       The dataset format is described below.  Note: the format of this\n",
      "     database was modified on 2/26/90 to conform with the format of all\n",
      "     the other databases in the UCI repository of machine learning databases.\n",
      " \n",
      " 5. Number of Instances: 3196 total\n",
      " \n",
      " 6. Number of Attributes: 36\n",
      " \n",
      " 7. Attribute Summaries:\n",
      "     Classes (2):  -- White-can-win (\"won\") and White-cannot-win (\"nowin\").\n",
      "           I believe that White is deemed to be unable to win if the Black pawn\n",
      "           can safely advance.\n",
      "     Attributes: see Shapiro's book.\n",
      " \n",
      " 8. Missing Attributes: --  none\n",
      " \n",
      " 9. Class Distribution:\n",
      "     In 1669 of the positions (52%), White can win.\n",
      "     In 1527 of the positions (48%), White cannot win.\n",
      " \n",
      " The format for instances in this database is a sequence of 37 attribute values.\n",
      " Each instance is a board-descriptions for this chess endgame.  The first\n",
      " 36 attributes describe the board.  The last (37th) attribute is the\n",
      " classification: \"win\" or \"nowin\".  There are 0 missing values.\n",
      " A typical board-description is\n",
      " \n",
      " f,f,f,f,f,f,f,f,f,f,f,f,l,f,n,f,f,t,f,f,f,f,f,f,f,t,f,f,f,f,f,f,f,t,t,n,won\n",
      " \n",
      " The names of the features do not appear in the board-descriptions.\n",
      " Instead, each feature correponds to a particular position in the\n",
      " feature-value list.  For example, the head of this list is the value\n",
      " for the feature \"bkblk\".  The following is the list of features, in\n",
      " the order in which their values appear in the feature-value list:\n",
      " \n",
      " [bkblk,bknwy,bkon8,bkona,bkspr,bkxbq,bkxcr,bkxwp,blxwp,bxqsq,cntxt,dsopp,dwipd,\n",
      "  hdchk,katri,mulch,qxmsq,r2ar8,reskd,reskr,rimmx,rkxwp,rxmsq,simpl,skach,skewr,\n",
      "  skrxp,spcop,stlmt,thrsk,wkcti,wkna8,wknck,wkovl,wkpos,wtoeg]\n",
      " \n",
      " In the file, there is one instance (board position) per line.\n",
      " \n",
      " \n",
      " Num Instances:     3196\n",
      " Num Attributes:    37\n",
      " Num Continuous:    0 (Int 0 / Real 0)\n",
      " Num Discrete:      37\n",
      " Missing values:    0 /  0.0%\n",
      "\n",
      "     name                      type enum ints real     missing    distinct  (1)\n",
      "   1 'bkblk'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "   2 'bknwy'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "   3 'bkon8'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "   4 'bkona'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "   5 'bkspr'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "   6 'bkxbq'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "   7 'bkxcr'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "   8 'bkxwp'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "   9 'blxwp'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "  10 'bxqsq'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "  11 'cntxt'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "  12 'dsopp'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "  13 'dwipd'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "  14 'hdchk'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "  15 'katri'                   Enum 100%   0%   0%     0 /  0%     3 /  0%   0% \n",
      "  16 'mulch'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "  17 'qxmsq'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "  18 'r2ar8'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "  19 'reskd'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "  20 'reskr'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "  21 'rimmx'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "  22 'rkxwp'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "  23 'rxmsq'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "  24 'simpl'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "  25 'skach'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "  26 'skewr'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "  27 'skrxp'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "  28 'spcop'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "  29 'stlmt'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "  30 'thrsk'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "  31 'wkcti'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "  32 'wkna8'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "  33 'wknck'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "  34 'wkovl'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "  35 'wkpos'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "  36 'wtoeg'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n",
      "  37 'class'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0%\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 41156: ada to file /home/pereirabarataap/.openml/cache/org/openml/www/datasets/41156/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41156\n",
      "The goal of this challenge is to expose the research community to real world datasets of interest to 4Paradigm. All datasets are formatted in a uniform way, though the type of data might differ. The data are provided as preprocessed matrices, so that participants can focus on classification, although participants are welcome to use additional feature extraction procedures (as long as they do not violate any rule of the challenge). All problems are binary classification problems and are assessed with the normalized Area Under the ROC Curve (AUC) metric (i.e. 2*AUC-1).\n",
      "                   The identity of the datasets and the type of data is concealed, though its structure is revealed. The final score in  phase 2 will be the average of rankings  on all testing datasets, a ranking will be generated from such results, and winners will be determined according to such ranking.\n",
      "                   The tasks are constrained by a time budget. The Codalab platform provides computational resources shared by all participants. Each code submission will be exceuted in a compute worker with the following characteristics: 2Cores / 8G Memory / 40G SSD with Ubuntu OS. To ensure the fairness of the evaluation, when a code submission is evaluated, its execution time is limited in time.\n",
      "                   http://automl.chalearn.org/data\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 44: spambase to file /home/pereirabarataap/.openml/cache/org/openml/www/datasets/44/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "**Author**: Mark Hopkins, Erik Reeber, George Forman, Jaap Suermondt    \n",
      "**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/spambase)   \n",
      "**Please cite**: [UCI](https://archive.ics.uci.edu/ml/citation_policy.html)\n",
      "\n",
      "SPAM E-mail Database  \n",
      "The \"spam\" concept is diverse: advertisements for products/websites, make money fast schemes, chain letters, pornography... Our collection of spam e-mails came from our postmaster and individuals who had filed spam.  Our collection of non-spam e-mails came from filed work and personal e-mails, and hence the word 'george' and the area code '650' are indicators of non-spam.  These are useful when constructing a personalized spam filter.  One would either have to blind such non-spam indicators or get a very wide collection of non-spam to generate a general purpose spam filter.\n",
      " \n",
      "For background on spam:  \n",
      "Cranor, Lorrie F., LaMacchia, Brian A.  Spam! Communications of the ACM, 41(8):74-83, 1998.  \n",
      "\n",
      "### Attribute Information:  \n",
      "The last column denotes whether the e-mail was considered spam (1) or not (0), i.e. unsolicited commercial e-mail. Most of the attributes indicate whether a particular word or character was frequently occurring in the e-mail. The run-length attributes (55-57) measure the length of sequences of consecutive capital letters.  \n",
      "\n",
      "For the statistical measures of each attribute, see the end of this file. Here are the definitions of the attributes:  \n",
      "\n",
      "48 continuous real [0,100] attributes of type  \n",
      "word_freq_WORD = percentage of words in the e-mail that match WORD,  i.e. 100 * (number of times the WORD appears in the e-mail) / total number of words in e-mail.  A \"word\" in this case is any string of alphanumeric characters bounded by non-alphanumeric characters or end-of-string.\n",
      " \n",
      "6 continuous real [0,100] attributes of type char_freq_CHAR = percentage of characters in the e-mail that match CHAR, i.e. 100 * (number of CHAR occurences) / total characters in e-mail\n",
      " \n",
      "1 continuous real [1,...] attribute of type capital_run_length_average\n",
      " = average length of uninterrupted sequences of capital letters\n",
      " \n",
      "1 continuous integer [1,...] attribute of type capital_run_length_longest\n",
      " = length of longest uninterrupted sequence of capital letters\n",
      " \n",
      "1 continuous integer [1,...] attribute of type capital_run_length_total\n",
      " = sum of length of uninterrupted sequences of capital letters\n",
      " = total number of capital letters in the e-mail\n",
      " \n",
      "1 nominal {0,1} class attribute of type spam\n",
      " = denotes whether the e-mail was considered spam (1) or not (0), \n",
      " i.e. unsolicited commercial e-mail.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 40701: churn to file /home/pereirabarataap/.openml/cache/org/openml/www/datasets/40701/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40701\n",
      "**Author**: Unknown  \n",
      "**Source**: [PMLB](https://github.com/EpistasisLab/penn-ml-benchmarks/tree/master/datasets/classification), [BigML](https://bigml.com/user/francisco/gallery/dataset/5163ad540c0b5e5b22000383), Supposedly from UCI but I can't find it there.  \n",
      "**Please cite**:   \n",
      "\n",
      "A dataset relating characteristics of telephony account features and usage and whether or not the customer churned. Originally used in [Discovering Knowledge in Data: An Introduction to Data Mining](http://secs.ac.in/wp-content/CSE_PORTAL/DataMining_Daniel.pdf).\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 979: waveform-5000 to file /home/pereirabarataap/.openml/cache/org/openml/www/datasets/979/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "979\n",
      "**Author**:   \n",
      "**Source**: Unknown - Date unknown  \n",
      "**Please cite**:   \n",
      "\n",
      "Binarized version of the original data set (see version 1). The multi-class target feature is converted to a two-class nominal target feature by re-labeling the majority class as positive ('P') and all others as negative ('N'). Originally converted by Quan Sun.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets_df = pd.DataFrame.from_dict(openml.datasets.list_datasets()).T.dropna(how=\"any\")\n",
    "\n",
    "loc = (datasets_df[\"NumberOfMissingValues\"]==0) & \\\n",
    "      (datasets_df[\"NumberOfClasses\"]==2) & \\\n",
    "      (datasets_df[\"MinorityClassSize\"]>=100) & \\\n",
    "      (datasets_df[\"NumberOfInstances\"]<=5000) & \\\n",
    "      (datasets_df[\"NumberOfFeatures\"]<=100) & \\\n",
    "      (datasets_df[\"NumberOfFeatures\"]>=20) & \\\n",
    "      ~(datasets_df[\"name\"].str.contains(\"_\"))\n",
    "\n",
    "dids = datasets_df.loc[loc].sort_values(by=[\"NumberOfInstances\"])[\"did\"]\n",
    "dids.tolist()\n",
    "\n",
    "for did in dids:\n",
    "    dataset = openml.datasets.get_dataset(did)\n",
    "    print(did)\n",
    "    print(dataset.description)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>did</th>\n",
       "      <th>name</th>\n",
       "      <th>version</th>\n",
       "      <th>uploader</th>\n",
       "      <th>status</th>\n",
       "      <th>format</th>\n",
       "      <th>MajorityClassSize</th>\n",
       "      <th>MaxNominalAttDistinctValues</th>\n",
       "      <th>MinorityClassSize</th>\n",
       "      <th>NumberOfClasses</th>\n",
       "      <th>NumberOfFeatures</th>\n",
       "      <th>NumberOfInstances</th>\n",
       "      <th>NumberOfNumericFeatures</th>\n",
       "      <th>NumberOfSymbolicFeatures</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>59</td>\n",
       "      <td>ionosphere</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>225</td>\n",
       "      <td>2</td>\n",
       "      <td>126</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>351</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>1063</td>\n",
       "      <td>kc2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>415</td>\n",
       "      <td>2</td>\n",
       "      <td>107</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>522</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1510</th>\n",
       "      <td>1510</td>\n",
       "      <td>wdbc</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>357</td>\n",
       "      <td>2</td>\n",
       "      <td>212</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>569</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40705</th>\n",
       "      <td>40705</td>\n",
       "      <td>tokyo1</td>\n",
       "      <td>1</td>\n",
       "      <td>869</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>613</td>\n",
       "      <td>2</td>\n",
       "      <td>346</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>959</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>credit-g</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>700</td>\n",
       "      <td>10</td>\n",
       "      <td>300</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>1000</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494</th>\n",
       "      <td>1494</td>\n",
       "      <td>qsar-biodeg</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>699</td>\n",
       "      <td>2</td>\n",
       "      <td>356</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>1055</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>1504</td>\n",
       "      <td>steel-plates-fault</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>1268</td>\n",
       "      <td>2</td>\n",
       "      <td>673</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>1941</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>1487</td>\n",
       "      <td>ozone-level-8hr</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>2374</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>2</td>\n",
       "      <td>73</td>\n",
       "      <td>2534</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>kr-vs-kp</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>1669</td>\n",
       "      <td>3</td>\n",
       "      <td>1527</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>3196</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>spambase</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>2788</td>\n",
       "      <td>2</td>\n",
       "      <td>1813</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>4601</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         did                name version uploader  status format  \\\n",
       "59        59          ionosphere       1        1  active   ARFF   \n",
       "1063    1063                 kc2       1        2  active   ARFF   \n",
       "1510    1510                wdbc       1       64  active   ARFF   \n",
       "40705  40705              tokyo1       1      869  active   ARFF   \n",
       "31        31            credit-g       1        1  active   ARFF   \n",
       "1494    1494         qsar-biodeg       1       64  active   ARFF   \n",
       "1504    1504  steel-plates-fault       1       64  active   ARFF   \n",
       "1487    1487     ozone-level-8hr       1       64  active   ARFF   \n",
       "3          3            kr-vs-kp       1        1  active   ARFF   \n",
       "44        44            spambase       1        1  active   ARFF   \n",
       "\n",
       "      MajorityClassSize MaxNominalAttDistinctValues MinorityClassSize  \\\n",
       "59                  225                           2               126   \n",
       "1063                415                           2               107   \n",
       "1510                357                           2               212   \n",
       "40705               613                           2               346   \n",
       "31                  700                          10               300   \n",
       "1494                699                           2               356   \n",
       "1504               1268                           2               673   \n",
       "1487               2374                           2               160   \n",
       "3                  1669                           3              1527   \n",
       "44                 2788                           2              1813   \n",
       "\n",
       "      NumberOfClasses NumberOfFeatures NumberOfInstances  \\\n",
       "59                  2               35               351   \n",
       "1063                2               22               522   \n",
       "1510                2               31               569   \n",
       "40705               2               45               959   \n",
       "31                  2               21              1000   \n",
       "1494                2               42              1055   \n",
       "1504                2               34              1941   \n",
       "1487                2               73              2534   \n",
       "3                   2               37              3196   \n",
       "44                  2               58              4601   \n",
       "\n",
       "      NumberOfNumericFeatures NumberOfSymbolicFeatures  \n",
       "59                         34                        1  \n",
       "1063                       21                        1  \n",
       "1510                       30                        1  \n",
       "40705                      42                        3  \n",
       "31                          7                       14  \n",
       "1494                       41                        1  \n",
       "1504                       33                        1  \n",
       "1487                       72                        1  \n",
       "3                           0                       37  \n",
       "44                         57                        1  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manual inspection yields the following 10 datasets\n",
    "dids = [59, 1063, 1510, 40705, 31, 1494, 1504, 1487, 3, 44]\n",
    "datasets_df.loc[dids].sort_values(by=[\"NumberOfInstances\"]).drop(columns=[\"NumberOfInstancesWithMissingValues\", \"NumberOfMissingValues\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature and Target Manipulation\n",
    "Before testing how different methods perform in crosslier detection, we need to generate crossliers to be detected.<br>\n",
    "The experimental setup can divided in:\n",
    "* symmetric\n",
    "    * both classes have the same proportion of crossliers\n",
    "* asymmetric\n",
    "    * the proportion of crossliers in both classes can be different\n",
    "\n",
    "#### Symmetric\n",
    "There are 2 parameters in this setup:\n",
    "* <font size=\"4\">$\\rho_y$</font> $\\in \\{.01,.02,.03,.04,.05,.06,.07,.08,.09,.1\\}$\n",
    "* <font size=\"4\">$\\rho_x$</font> $\\in \\{0,  .05, .1,.15, .2, .25, .3, .35, .4, .45\\}$\n",
    "\n",
    "<font size=\"4\">$\\rho_y$</font> represents the proportion of instances with labels swapped in each class.<br>\n",
    "<font size=\"4\">$\\rho_x$</font> represents the proportion of attributes of which the values are replaced in label-swapped instances.<br>\n",
    "\n",
    "#### Asymmetric\n",
    "There are 4 parameters in this setup:\n",
    "* <font size=\"4\">$\\rho_{y+}$</font> $\\in \\{.01, .02,.03, .04,.05, .06,.07, .08,.09,  .1\\}$\n",
    "* <font size=\"4\">$\\rho_{y-}$</font> $\\in \\{.01, .02,.03, .04,.05, .06,.07, .08,.09,  .1\\}$\n",
    "* <font size=\"4\">$\\rho_{x+}$</font> $\\in \\{0,   .05, .1, .15, .2, .25, .3, .35, .4, .45\\}$\n",
    "* <font size=\"4\">$\\rho_{x-}$</font> $\\in \\{0,   .05, .1, .15, .2, .25, .3, .35, .4, .45\\}$\n",
    "\n",
    "<font size=\"4\">$\\rho_{y+}$</font> represents the proportion of instances with labels swapped: they originally belong to the positive class.<br>\n",
    "<font size=\"4\">$\\rho_{y-}$</font> represents the proportion of instances with labels swapped: they originally belong to the negative class.<br>\n",
    "<font size=\"4\">$\\rho_{x+}$</font> represents the proportion of attributes of which the values are replaced in label-swapped instances: the instances originally belong to the positive class.<br>\n",
    "<font size=\"4\">$\\rho_{x-}$</font> represents the proportion of attributes of which the values are replaced in label-swapped instances: the instances originally belong to the negative class.<br>\n",
    "\n",
    "The replacement simulates real-world fraud in which manipulation of feature values towards those of another label further masks the real label.<br>\n",
    "Replacement values are drawn from <b>univariate</b> distributions with parameters estimated from the respective attributes belonging to the class being mimicked.<br>\n",
    "To note, for both <i>symmetric</i> and <i>asymmetric</i> setups:\n",
    "* Each dataset will have <b>50</b> random initialisations for each set of parameters\n",
    "* Attributes to be replaced:\n",
    "    * are chosen by either:\n",
    "        * <b>random</b> selection, where attributes are selected randomly for each instance\n",
    "        * <b>category</b> selection, where each class has a set of random attributes to mimic (1 set of attributes per class)\n",
    "        * <b>consistent</b> selection, where a single set of random attributes are selected for all label-swapped instances regardless of class\n",
    "    * are modelled as:\n",
    "        * normal distributions $\\mathcal{N}(\\hat{\\mu}, \\hat{\\sigma})$ for numerical attributes\n",
    "        * multinomial distributions for categorical attributes\n",
    "    * have distribution parameters either:\n",
    "        * <b>clean</b>, estimated <i>a priori</i> label swaps\n",
    "        * <b>noisy</b>, estimated <i>a posteriori</i> label swaps\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_apply(y):\n",
    "    # this function is required by XGBoost\n",
    "    # since it cannot handle dummy_variables well\n",
    "    # dummy names cannot contain \"<\" nor \",\"\n",
    "    try:\n",
    "        return (y.replace(\"<\", \"lt\")).replace(\",\", \"c\")\n",
    "    except:\n",
    "        return y\n",
    "    \n",
    "def parallel_symmetric_setup(seed, did_folder, X, y, df, attribute_names, categorical_indicator, y_pos, y_neg, X_clean_pos, X_clean_neg):\n",
    "    seed_folder = did_folder + \"seed=\" + str(seed) + \"/\"\n",
    "    try:\n",
    "        os.mkdir(seed_folder)\n",
    "    except:\n",
    "        shutil.rmtree(seed_folder)\n",
    "        os.mkdir(seed_folder)\n",
    "    np.random.seed(seed)\n",
    "    for roh_y_prct in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "        roh_y_folder = seed_folder + \"roh_y=\" + str(roh_y_prct) + \"/\"\n",
    "        try:\n",
    "            os.mkdir(roh_y_folder)\n",
    "        except:\n",
    "            shutil.rmtree(roh_y_folder)\n",
    "            os.mkdir(roh_y_folder)\n",
    "        roh_y = roh_y_prct/100\n",
    "        # number of instances to label swap\n",
    "        roh_y_pos_n = int(np.ceil(roh_y*X_clean_pos.shape[0]))\n",
    "        roh_y_neg_n = int(np.ceil(roh_y*X_clean_neg.shape[0]))\n",
    "        for roh_x_prct in [0, 5, 10, 15, 20, 25, 30, 35, 40, 45]:\n",
    "            roh_x_folder = roh_y_folder + \"roh_x=\" + str(roh_x_prct) + \"/\"\n",
    "            try:\n",
    "                os.mkdir(roh_x_folder)\n",
    "            except:\n",
    "                shutil.rmtree(roh_x_folder)\n",
    "                os.mkdir(roh_x_folder)\n",
    "            roh_x = roh_x_prct/100\n",
    "            # number of attributes to swap\n",
    "            roh_x_n = int(np.ceil(roh_x*X.shape[1]))\n",
    "            # label-swap instance locations (index)\n",
    "            roh_y_pos_loc = np.random.choice(X_clean_pos.index, roh_y_pos_n, replace=False) # will be present in X_noisy_neg\n",
    "            roh_y_neg_loc = np.random.choice(X_clean_neg.index, roh_y_neg_n, replace=False) # will be present in X_noisy_pos    \n",
    "            if roh_x_n == 0:\n",
    "                # no attribute replacement is required, only label swap\n",
    "                corrupted_df = copy(df)\n",
    "                corrupted_df.loc[roh_y_pos_loc, \"class\"] = y_neg\n",
    "                corrupted_df.loc[roh_y_neg_loc, \"class\"] = y_pos\n",
    "                # saving corrupted df\n",
    "                joblib.dump(corrupted_df, roh_x_folder + \"corrupted_df.pkl\")\n",
    "                corrupted_df.to_csv(roh_x_folder + \"corrupted_df.csv\", index=False)\n",
    "            else:\n",
    "                X_noisy_pos = pd.concat((\n",
    "                    copy(X.loc[roh_y_neg_loc]), # getting swap examples from other class\n",
    "                    copy(X_clean_pos.drop(labels=roh_y_pos_loc)) # dropping examples that are given to other class\n",
    "                ))\n",
    "                X_noisy_neg = pd.concat((\n",
    "                    copy(X.loc[roh_y_pos_loc]), # getting swap examples from other class\n",
    "                    copy(X_clean_neg.drop(labels=roh_y_neg_loc)) # dropping examples that are given to other class\n",
    "                ))\n",
    "                # random selection\n",
    "                # each label-swapped instance has its own set of random attributes to replace\n",
    "                random_folder = roh_x_folder + \"random/\"\n",
    "                try:\n",
    "                    os.mkdir(random_folder)\n",
    "                except:\n",
    "                    shutil.rmtree(random_folder)\n",
    "                    os.mkdir(random_folder)\n",
    "                random_clean_X = copy(X)\n",
    "                random_noisy_X = copy(X)\n",
    "                for roh_y_pos_index in roh_y_pos_loc:\n",
    "                    replace_attribute_indexs = np.random.choice(range(X.shape[1]), roh_x_n, replace=False)\n",
    "                    replace_attribute_names = attribute_names[replace_attribute_indexs]\n",
    "                    replace_categorical_indicators = categorical_indicator[replace_attribute_indexs]\n",
    "                    # for each attribute being replaced\n",
    "                    clean_replacement_values = []\n",
    "                    noisy_replacement_values = []\n",
    "                    for i in range(roh_x_n):\n",
    "                        replace_attribute_name = replace_attribute_names[i]\n",
    "                        replace_categorical_indicator = replace_categorical_indicators[i]\n",
    "                        if replace_categorical_indicator:\n",
    "                            # is categorical\n",
    "                            clean_replacement_value = np.random.choice(X_clean_neg[replace_attribute_name], 1)\n",
    "                            noisy_replacement_value = np.random.choice(X_noisy_neg[replace_attribute_name], 1)\n",
    "                        else:\n",
    "                            # is numerical\n",
    "                            clean_mu = np.mean(X_clean_neg[replace_attribute_name])\n",
    "                            clean_sig = np.std(X_clean_neg[replace_attribute_name], ddof=1)\n",
    "                            clean_replacement_value = np.random.normal(clean_mu, clean_sig, 1)\n",
    "                            noisy_mu = np.mean(X_noisy_neg[replace_attribute_name])\n",
    "                            noisy_sig = np.std(X_noisy_neg[replace_attribute_name], ddof=1)\n",
    "                            noisy_replacement_value = np.random.normal(noisy_mu, noisy_sig, 1)\n",
    "                        clean_replacement_values += [clean_replacement_value[0]]\n",
    "                        noisy_replacement_values += [noisy_replacement_value[0]]\n",
    "                    random_clean_X.loc[roh_y_pos_index, replace_attribute_names] = clean_replacement_values\n",
    "                    random_noisy_X.loc[roh_y_pos_index, replace_attribute_names] = noisy_replacement_values\n",
    "                for roh_y_neg_index in roh_y_neg_loc:\n",
    "                    replace_attribute_indexs = np.random.choice(range(X.shape[1]), roh_x_n, replace=False)\n",
    "                    replace_attribute_names = attribute_names[replace_attribute_indexs]\n",
    "                    replace_categorical_indicators = categorical_indicator[replace_attribute_indexs]\n",
    "                    # for each attribute being replaced\n",
    "                    clean_replacement_values = []\n",
    "                    noisy_replacement_values = []\n",
    "                    for i in range(roh_x_n):\n",
    "                        replace_attribute_name = replace_attribute_names[i]\n",
    "                        replace_categorical_indicator = replace_categorical_indicators[i]\n",
    "                        if replace_categorical_indicator:\n",
    "                            # is categorical\n",
    "                            clean_replacement_value = np.random.choice(X_clean_pos[replace_attribute_name], 1)\n",
    "                            noisy_replacement_value = np.random.choice(X_noisy_pos[replace_attribute_name], 1)\n",
    "                        else:\n",
    "                            # is numerical\n",
    "                            clean_mu = np.mean(X_clean_pos[replace_attribute_name])\n",
    "                            clean_sig = np.std(X_clean_pos[replace_attribute_name], ddof=1)\n",
    "                            clean_replacement_value = np.random.normal(clean_mu, clean_sig, 1)\n",
    "                            noisy_mu = np.mean(X_noisy_pos[replace_attribute_name])\n",
    "                            noisy_sig = np.std(X_noisy_pos[replace_attribute_name], ddof=1)\n",
    "                            noisy_replacement_value = np.random.normal(noisy_mu, noisy_sig, 1)\n",
    "                        clean_replacement_values += [clean_replacement_value[0]]\n",
    "                        noisy_replacement_values += [noisy_replacement_value[0]]\n",
    "                    random_clean_X.loc[roh_y_neg_index, replace_attribute_names] = clean_replacement_values\n",
    "                    random_noisy_X.loc[roh_y_neg_index, replace_attribute_names] = noisy_replacement_values\n",
    "                corrupted_clean_df = pd.concat((copy(random_clean_X), copy(y)), axis=1)\n",
    "                corrupted_clean_df.loc[roh_y_pos_loc, \"class\"] = y_neg\n",
    "                corrupted_clean_df.loc[roh_y_neg_loc, \"class\"] = y_pos\n",
    "                corrupted_noisy_df = pd.concat((copy(random_noisy_X), copy(y)), axis=1)\n",
    "                corrupted_noisy_df.loc[roh_y_pos_loc, \"class\"] = y_neg\n",
    "                corrupted_noisy_df.loc[roh_y_neg_loc, \"class\"] = y_pos\n",
    "                joblib.dump(corrupted_clean_df, random_folder + \"corrupted_clean_df.pkl\")\n",
    "                joblib.dump(corrupted_noisy_df, random_folder + \"corrupted_noisy_df.pkl\")\n",
    "                corrupted_clean_df.to_csv(random_folder + \"corrupted_clean_df.csv\", index=False)\n",
    "                corrupted_noisy_df.to_csv(random_folder + \"corrupted_noisy_df.csv\", index=False)\n",
    "\n",
    "                # category selection\n",
    "                # each class has a set of random attributes to mimic (1 set of attributes per class)\n",
    "                category_folder = roh_x_folder + \"category/\"\n",
    "                try:\n",
    "                    os.mkdir(category_folder)\n",
    "                except:\n",
    "                    shutil.rmtree(category_folder)\n",
    "                    os.mkdir(category_folder)\n",
    "                category_clean_X = copy(X)\n",
    "                category_noisy_X = copy(X)\n",
    "                replace_attribute_indexs = np.random.choice(range(X.shape[1]), roh_x_n, replace=False)\n",
    "                replace_attribute_names = attribute_names[replace_attribute_indexs]\n",
    "                replace_categorical_indicators = categorical_indicator[replace_attribute_indexs]\n",
    "                for roh_y_pos_index in roh_y_pos_loc:\n",
    "                    clean_replacement_values = []\n",
    "                    noisy_replacement_values = []\n",
    "                    for i in range(roh_x_n):\n",
    "                        replace_attribute_name = replace_attribute_names[i]\n",
    "                        replace_categorical_indicator = replace_categorical_indicators[i]\n",
    "                        if replace_categorical_indicator:\n",
    "                            # is categorical\n",
    "                            clean_replacement_value = np.random.choice(X_clean_neg[replace_attribute_name], 1)\n",
    "                            noisy_replacement_value = np.random.choice(X_noisy_neg[replace_attribute_name], 1)\n",
    "                        else:\n",
    "                            # is numerical\n",
    "                            clean_mu = np.mean(X_clean_neg[replace_attribute_name])\n",
    "                            clean_sig = np.std(X_clean_neg[replace_attribute_name], ddof=1)\n",
    "                            clean_replacement_value = np.random.normal(clean_mu, clean_sig, 1)\n",
    "                            noisy_mu = np.mean(X_noisy_neg[replace_attribute_name])\n",
    "                            noisy_sig = np.std(X_noisy_neg[replace_attribute_name], ddof=1)\n",
    "                            noisy_replacement_value = np.random.normal(noisy_mu, noisy_sig, 1)\n",
    "                        clean_replacement_values += [clean_replacement_value[0]]\n",
    "                        noisy_replacement_values += [noisy_replacement_value[0]]\n",
    "                    category_clean_X.loc[roh_y_pos_index, replace_attribute_names] = clean_replacement_values\n",
    "                    category_noisy_X.loc[roh_y_pos_index, replace_attribute_names] = noisy_replacement_values                    \n",
    "                replace_attribute_indexs = np.random.choice(range(X.shape[1]), roh_x_n, replace=False)\n",
    "                replace_attribute_names = attribute_names[replace_attribute_indexs]\n",
    "                replace_categorical_indicators = categorical_indicator[replace_attribute_indexs]\n",
    "                for roh_y_neg_index in roh_y_neg_loc:\n",
    "                    clean_replacement_values = []\n",
    "                    noisy_replacement_values = []\n",
    "                    for i in range(roh_x_n):\n",
    "                        replace_attribute_name = replace_attribute_names[i]\n",
    "                        replace_categorical_indicator = replace_categorical_indicators[i]\n",
    "                        if replace_categorical_indicator:\n",
    "                            # is categorical\n",
    "                            clean_replacement_value = np.random.choice(X_clean_pos[replace_attribute_name], 1)\n",
    "                            noisy_replacement_value = np.random.choice(X_noisy_pos[replace_attribute_name], 1)\n",
    "                        else:\n",
    "                            # is numerical\n",
    "                            clean_mu = np.mean(X_clean_pos[replace_attribute_name])\n",
    "                            clean_sig = np.std(X_clean_pos[replace_attribute_name], ddof=1)\n",
    "                            clean_replacement_value = np.random.normal(clean_mu, clean_sig, 1)\n",
    "                            noisy_mu = np.mean(X_noisy_pos[replace_attribute_name])\n",
    "                            noisy_sig = np.std(X_noisy_pos[replace_attribute_name], ddof=1)\n",
    "                            noisy_replacement_value = np.random.normal(noisy_mu, noisy_sig, 1)\n",
    "                        clean_replacement_values += [clean_replacement_value[0]]\n",
    "                        noisy_replacement_values += [noisy_replacement_value[0]]\n",
    "                    category_clean_X.loc[roh_y_neg_index, replace_attribute_names] = clean_replacement_values\n",
    "                    category_noisy_X.loc[roh_y_neg_index, replace_attribute_names] = noisy_replacement_values\n",
    "                corrupted_clean_df = pd.concat((copy(category_clean_X), copy(y)), axis=1)\n",
    "                corrupted_clean_df.loc[roh_y_pos_loc, \"class\"] = y_neg\n",
    "                corrupted_clean_df.loc[roh_y_neg_loc, \"class\"] = y_pos\n",
    "                corrupted_noisy_df = pd.concat((copy(category_noisy_X), copy(y)), axis=1)\n",
    "                corrupted_noisy_df.loc[roh_y_pos_loc, \"class\"] = y_neg\n",
    "                corrupted_noisy_df.loc[roh_y_neg_loc, \"class\"] = y_pos\n",
    "                joblib.dump(corrupted_clean_df, category_folder + \"corrupted_clean_df.pkl\")\n",
    "                joblib.dump(corrupted_noisy_df, category_folder + \"corrupted_noisy_df.pkl\")\n",
    "                corrupted_clean_df.to_csv(category_folder + \"corrupted_clean_df.csv\", index=False)\n",
    "                corrupted_noisy_df.to_csv(category_folder + \"corrupted_noisy_df.csv\", index=False)\n",
    "\n",
    "                # consistent selection\n",
    "                # same attributes are selected for every label-swapped instance regardless of class\n",
    "                consistent_folder = roh_x_folder + \"consistent/\"\n",
    "                try:\n",
    "                    os.mkdir(consistent_folder)\n",
    "                except:\n",
    "                    shutil.rmtree(consistent_folder)\n",
    "                    os.mkdir(consistent_folder)\n",
    "                consistent_clean_X = copy(X)\n",
    "                consistent_noisy_X = copy(X)\n",
    "                replace_attribute_indexs = np.random.choice(range(X.shape[1]), roh_x_n, replace=False)\n",
    "                replace_attribute_names = attribute_names[replace_attribute_indexs]\n",
    "                replace_categorical_indicators = categorical_indicator[replace_attribute_indexs]\n",
    "                for roh_y_pos_index in roh_y_pos_loc:\n",
    "                    clean_replacement_values = []\n",
    "                    noisy_replacement_values = []\n",
    "                    for i in range(roh_x_n):\n",
    "                        replace_attribute_name = replace_attribute_names[i]\n",
    "                        replace_categorical_indicator = replace_categorical_indicators[i]\n",
    "                        if replace_categorical_indicator:\n",
    "                            # is categorical\n",
    "                            clean_replacement_value = np.random.choice(X_clean_neg[replace_attribute_name], 1)\n",
    "                            noisy_replacement_value = np.random.choice(X_noisy_neg[replace_attribute_name], 1)\n",
    "                        else:\n",
    "                            # is numerical\n",
    "                            clean_mu = np.mean(X_clean_neg[replace_attribute_name])\n",
    "                            clean_sig = np.std(X_clean_neg[replace_attribute_name], ddof=1)\n",
    "                            clean_replacement_value = np.random.normal(clean_mu, clean_sig, 1)\n",
    "                            noisy_mu = np.mean(X_noisy_neg[replace_attribute_name])\n",
    "                            noisy_sig = np.std(X_noisy_neg[replace_attribute_name], ddof=1)\n",
    "                            noisy_replacement_value = np.random.normal(noisy_mu, noisy_sig, 1)\n",
    "                        clean_replacement_values += [clean_replacement_value[0]]\n",
    "                        noisy_replacement_values += [noisy_replacement_value[0]]\n",
    "                    consistent_clean_X.loc[roh_y_pos_index, replace_attribute_names] = clean_replacement_values\n",
    "                    consistent_noisy_X.loc[roh_y_pos_index, replace_attribute_names] = noisy_replacement_values                    \n",
    "                for roh_y_neg_index in roh_y_neg_loc:\n",
    "                    clean_replacement_values = []\n",
    "                    noisy_replacement_values = []\n",
    "                    for i in range(roh_x_n):\n",
    "                        replace_attribute_name = replace_attribute_names[i]\n",
    "                        replace_categorical_indicator = replace_categorical_indicators[i]\n",
    "                        if replace_categorical_indicator:\n",
    "                            # is categorical\n",
    "                            clean_replacement_value = np.random.choice(X_clean_pos[replace_attribute_name], 1)\n",
    "                            noisy_replacement_value = np.random.choice(X_noisy_pos[replace_attribute_name], 1)\n",
    "                        else:\n",
    "                            # is numerical\n",
    "                            clean_mu = np.mean(X_clean_pos[replace_attribute_name])\n",
    "                            clean_sig = np.std(X_clean_pos[replace_attribute_name], ddof=1)\n",
    "                            clean_replacement_value = np.random.normal(clean_mu, clean_sig, 1)\n",
    "                            noisy_mu = np.mean(X_noisy_pos[replace_attribute_name])\n",
    "                            noisy_sig = np.std(X_noisy_pos[replace_attribute_name], ddof=1)\n",
    "                            noisy_replacement_value = np.random.normal(noisy_mu, noisy_sig, 1)\n",
    "                        clean_replacement_values += [clean_replacement_value[0]]\n",
    "                        noisy_replacement_values += [noisy_replacement_value[0]]\n",
    "                    consistent_clean_X.loc[roh_y_neg_index, replace_attribute_names] = clean_replacement_values\n",
    "                    consistent_noisy_X.loc[roh_y_neg_index, replace_attribute_names] = noisy_replacement_values\n",
    "                corrupted_clean_df = pd.concat((copy(consistent_clean_X), copy(y)), axis=1)\n",
    "                corrupted_clean_df.loc[roh_y_pos_loc, \"class\"] = y_neg\n",
    "                corrupted_clean_df.loc[roh_y_neg_loc, \"class\"] = y_pos\n",
    "                corrupted_noisy_df = pd.concat((copy(consistent_noisy_X), copy(y)), axis=1)\n",
    "                corrupted_noisy_df.loc[roh_y_pos_loc, \"class\"] = y_neg\n",
    "                corrupted_noisy_df.loc[roh_y_neg_loc, \"class\"] = y_pos\n",
    "                joblib.dump(corrupted_clean_df, consistent_folder + \"corrupted_clean_df.pkl\")\n",
    "                joblib.dump(corrupted_noisy_df, consistent_folder + \"corrupted_noisy_df.pkl\")\n",
    "                corrupted_clean_df.to_csv(consistent_folder + \"corrupted_clean_df.csv\", index=False)\n",
    "                corrupted_noisy_df.to_csv(consistent_folder + \"corrupted_noisy_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e16ee8ca934dbdbc251921c0ac619d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# SYMETRIC\n",
    "# SETUP\n",
    "data_folder = \"/data/pereirabarataap/journal/symmetric/dids/\"\n",
    "try:\n",
    "    os.mkdir(data_folder)\n",
    "except:\n",
    "    shutil.rmtree(data_folder)\n",
    "    os.mkdir(data_folder)\n",
    "seeds = range(50)\n",
    "for did in tqdm(dids):\n",
    "    did_folder = data_folder + \"did=\" + str(did) + \"/\"\n",
    "    try:\n",
    "        os.mkdir(did_folder)\n",
    "    except:\n",
    "        shutil.rmtree(did_folder)\n",
    "        os.mkdir(did_folder)\n",
    "    dataset = openml.datasets.get_dataset(did)\n",
    "    X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "        dataset_format='dataframe',\n",
    "        target=dataset.default_target_attribute\n",
    "    )\n",
    "    categorical_indicator = np.array(categorical_indicator)\n",
    "    X = X.apply(lambda x: x.apply(lambda y: in_apply(y)))\n",
    "    attribute_names = np.array(range(X.shape[1])).astype(str)\n",
    "    X.columns = attribute_names\n",
    "    df = pd.concat((copy(X), copy(y)), axis=1)\n",
    "    df.columns = attribute_names.tolist() + [\"class\"]\n",
    "    y = copy(df[\"class\"])\n",
    "    \n",
    "    joblib.dump(df, did_folder+\"df.pkl\")\n",
    "    df.to_csv(did_folder+\"df.csv\", index=False)\n",
    "    \n",
    "    y_pos, y_neg = y.unique()\n",
    "    X_clean_pos = copy(X.loc[y==y_pos]) # clean positive attributes\n",
    "    X_clean_neg = copy(X.loc[y==y_neg]) # clean negative attributes\n",
    "    \n",
    "    Parallel(n_jobs=len(seeds), backend=\"loky\")(delayed(parallel_symmetric_setup)(\n",
    "        seed=copy(seed),\n",
    "        did_folder=copy(did_folder),\n",
    "        X=copy(X),\n",
    "        y=copy(y),\n",
    "        df=copy(df),\n",
    "        attribute_names=copy(attribute_names),\n",
    "        categorical_indicator=copy(categorical_indicator),\n",
    "        y_pos=copy(y_pos),\n",
    "        y_neg=copy(y_neg),\n",
    "        X_clean_pos=copy(X_clean_pos),\n",
    "        X_clean_neg=copy(X_clean_neg),\n",
    "    ) for seed in seeds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
